\section{LLM - Connection}
Before the user gets to actually answer questions provided by the LLM, we need to submit our age and experience, if we did not already do so.
Only then the questions will start to get fetched from the LLM API.
But this is not happening directly, for that we also implemented a little backend, that needs to be started, which then handles all needed API calls.
Currently though the backend is not protected against calls from outside, so it is not recommended to run it on a public server.
One might first want to implement some kind of authentication, before doing so. 
The same goes for the LLM API, as it is a Ollama Backend that runns openly.
Here we also recommend to implement authentication procedures such as private keys or doing network segmentation only for the API to prevent unwanted access.

\subsection{Tips}
To catch a potential users attention, we decided to use Tips and eye catching messages on our main screen.
This shall then make the users interest about our PSUI rise and let him or her approach our Public Display.
These tips are getting fetched from the LLM API.
Incase of server connection issues, we have a fallback mechanism that uses a locally saved tips.
Later on in the future, one may also add not only tips but also current news in cybersecurity and generally speaking of security awareness.

\subsection{Questions}
Here we differentiate between the two modes as one of them has a userId and the other one does not.
Considering that, lets first look at the guest-mode question api request.
For that we have prepared a prompt and a list of topics from which the llm can choose.
Both of them are stored in the backend folder.
From here on we directly pass arguments into the prompt such as age, experience and the topic.
When we are done, we simply send over the whole prompt to the LLM API, which then responds with a specific json format.
The json format is then parsed and send over to the host front-end.
Formatted the json represents this structure:
\begin{lstlisting}[language=json,firstnumber=1]
{
    "question": ... ,
    "option_s": ... ,
    "dropZones": ... ,
    "correctAnswer_s": ... ,
    "topic": ... ,
    "questionType": ...
}
\end{lstlisting}
Each one having there own meaning.
\begin{itemize}
    \item \textbf{question}: The question that the user has to answer (String).
    \item \textbf{option\_s}: The options that the user can choose from (String array).
    \item \textbf{dropZones}: Represent drop zones in a drag \& drop event, where the user can drag and drop the options (String array).
    \item \textbf{correctAnswer\_s}: Correct answers for the question (String array).
    \item \textbf{topic}: The overall topic of the question (String).
    \item \textbf{questionType}: The event-type of the question (String).
\end{itemize}

\subsection{Explanation}
In the explanation we actually dont need to differentiate between the two modes, as the explanation should be universal.
There may be a critical moment where one might think to add some extra explanation features for the private mode but we did not implement that.
As we have fetched a question from the LLM API we can refeed it with the almost same provided json format.
In our case we only need the question and the correctAnswer\_s aswell as a boolean telling the LLM we did indeed answer the question incorrectly. \\
When we do answer correctly there is no need to explain the question as the user already knew the answer (hopefully did not guess).
This means we have a simple return format for our explanation, because in the end we only need the explanation text. \\
Therefore our json format looks like this:
\begin{lstlisting}[language=json,firstnumber=1]
{
    "explanation": ... 
}
\end{lstlisting}
Where the \textbf{explanation} (String) is the text that the LLM provides us with.

\subsection{Saving Answers}
When the user enters the guest mode, we do not save anything but rather do a local session with grading at the end.
However, when the user enters the private mode, we do save the answers so that his or her previously answered questions can be taken into account.
This makes the LLM way more personalized and the user can also see his or her progress eventually.
The main idea is, that we get the questionType from the LLM API.
After that we add to our autogenerated file with the userId as its filename the current questionType.
For the final part, we increase a counter for the correctly answered questions according to the questionType.
If the user indeed answered the question correctly, we increment by one, otherwise not.
The total amount of questions is also saved for each questionType.
This amount always gets incremented by one, no matter if the user answered correctly or not. \\
In general a progress output may look like this:
\begin{lstlisting}[language=json,firstnumber=1]
{
    ... ,
    "progress": {
        "passwords": {
            "correct": 0,
            "total": 1
        },
        "camera_microphone_access": {
            "correct": 0,
            "total": 1
        },
        "kids_online_safety": {
            "correct": 2,
            "total": 2
        }
  }
}
\end{lstlisting}
Which would mean that in total the user has answered 4 questions, of which he answered 2 correctly.

\subsection{Feedback}
The moment the user decides that he or she has had enough of the questions, one can request a feedback.
This feedback is then generated by the LLM API and sent over to the front-end.
The user has now the oppertunity to read the feedback and then decide for himself learning more about a topic or not.
Feedbackwise we offer a spider chart limited to the maximum amount of 8 topics, where we define from 0 to a 100 percent how well the user did in each topic.
Aswell as a specificly generated text, that gives the user a more detailed overview of his or her performance.
Afterwards the user can disconnect from the session or do nothing as the session will automatically disconnect after a certain amount of time.

\section{Anti-Breaching Sessions}
As every server, that is reachable from the internet or inside a local network, we need to make sure, that we are as secure as possible.
For that we implemented a simple anti-breaching session logic.
Everytime a user connects to the backend we check if someone is already connected (max amount of client: 1, host: 1).
Therefore moving the unwanted user into a url sinkhole.
This means that refreshing the page will keep you on a prohibited url where one cannot do anything, which eases the backend servers capacity.
Additionally to that we have a structure, where even if an attacker gets the information about the current session, he cannot make anything of it.
The actual data that is stored from the user does not contain any sensitive information, 
but rather a unique userId, a rough approximate of the age aswell as the experience level, the user provided us with.
Additionally the JWT token is only valid for a certain amount of time, which makes it even harder to breach the session.
Though our application is not perfect and we do not claim it to be, we still try to make it as secure as possible.
One can for example try to impersonate a user by storing the userId in his or her own local storage, this would mean, that the attacker may use the interface as
the pronounced user.
However the attacker also has to be motivated to then answer questions for the user, which he or her is impersonating.
This has no actual benefit for the attacker.
We also do something that is almost bullet proof, so that even if an attacker mages to connect to the session there is nothing the attacker can do.
As the QR Code is generated the user scans it.
In that Moment the file is being created or reused from the connected user.
Even if the attacker tries to connect now he or her will be send to the sinkhole as we remember the first user that connected with his or her specific userId.
Therefore making sure the file of the user is safe and not touchable by a potential attacker.

\section{File Cleansing}
As the user logs into the private mode, we create a file, but how do we remove it again?
There are alot of ways one could do so, but most of them require some interaction.
Therfore we decided that a simple python script, running maybe once a month, shall remove all files that are older than 30 days.
This may seem unfortunate for the user as his progress is lost after one month but for the servers memory sake we need a way to remove files after a certain amount of time.
If we try to keep them forever this may lead to dead files, as users may never return or users often delete there browser cache, maybe even change there browser.
All that has influence on how the files are getting accessed and therefore we really need a way to remove them.
The script itself is in the backend folder called ''checkFiles.py''.